{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warren_tweets = pickle.load(open('./tweetsent/tweets/old/warren_tweets_old.pkl', 'rb'))\n",
    "warren_senti_scores = pickle.load(open('./tweetsent/senti_scores/warren_senti_scores.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Thresholds\n",
    "num_retweets_warren = np.array([warren_tweets[i]['retweet_count']\n",
    "                                for i in range(len(warren_tweets))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding max character\n",
    "warren_max_char=0 #316\n",
    "for i in warren_tweets[0:]:\n",
    "    warren_max_char = max(warren_max_char, warren_senti_scores[i['full_text']]['usage']['text_characters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a corpus\n",
    "corpus = set()\n",
    "for tweet in warren_senti_scores:\n",
    "    corpus.update({i['text'] for i in warren_senti_scores[tweet]['keywords']})\n",
    "warren_sorted_corpus = sorted(corpus)\n",
    "\n",
    "with open('FeatureData/warren_corpus.pk', 'wb') as file:\n",
    "    pickle.dump(warren_sorted_corpus, file)\n",
    "\n",
    "with open('FeatureData/warren_corpus.pk', 'rb') as file:\n",
    "    warren_sorted_corpus = pickle.load(file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5712"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(warren_sorted_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Feature Matrix\n",
    "warren_features = []\n",
    "warren_labels = []\n",
    "warren_feature_names = ['sadness', 'joy', 'fear', 'disgust', 'anger',\n",
    "                        'sentiment', 'character'] + [i for i in warren_sorted_corpus]\n",
    "\n",
    "for i in warren_tweets:\n",
    "    # Ambigious discarded Binary Labels\n",
    "    if i['retweet_count'] <= 1083: #1083\n",
    "        warren_labels.append(-1)\n",
    "    elif i['retweet_count'] >= 1614: #1614\n",
    "        warren_labels.append(1)\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    # Feature\n",
    "    tweet_feature = []\n",
    "    for j,k in warren_senti_scores[i['full_text']]['emotion']['document']['emotion'].items():\n",
    "        tweet_feature.append(k)\n",
    "    tweet_feature.append(warren_senti_scores[i['full_text']]['sentiment']['document']['score'])\n",
    "    warren_feature_names.append('sentiment')\n",
    "    \n",
    "    tweet_feature.append(warren_senti_scores[i['full_text']]['usage']['text_characters']/warren_max_char)\n",
    "    warren_feature_names.append('character')\n",
    "    \n",
    "    # One-hot Encoded Features\n",
    "    text_relevance = dict({sent['text']:sent['relevance'] for sent in warren_senti_scores[i['full_text']]['keywords']})\n",
    "    tweet_onehot=[]\n",
    "    for keys in warren_sorted_corpus:\n",
    "        \n",
    "        tweet_onehot.append(0 if keys not in text_relevance.keys() else text_relevance[keys])\n",
    "    tweet_feature.extend(tweet_onehot)\n",
    "    \n",
    "    # Add all to features matrix\n",
    "    warren_features.append(tweet_feature)\n",
    "\n",
    "with open('FeatureData/warren_features.pk', 'wb') as file:\n",
    "    pickle.dump([warren_features, warren_feature_names, warren_labels], file)\n",
    "with open('FeatureData/warren_features.pk', 'rb') as file:\n",
    "    warren_features, warren_feature_names, warren_labels = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(warren_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_warren, X_test_warren, y_train_warren, y_test_warren = train_test_split(warren_features, warren_labels, test_size=1/3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6771929824561403\n",
      "0.9828767123287672\n"
     ]
    }
   ],
   "source": [
    "lr_warren = LogisticRegression(C=2.0)\n",
    "lr_warren.fit(X_train_warren, y_train_warren)\n",
    "lr_warren.score(X_test_warren, y_test_warren)\n",
    "print(f1_score(lr_warren.predict(X_test_warren), y_test_warren))\n",
    "print(f1_score(lr_warren.predict(X_train_warren), y_train_warren))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "warren_train_acc = lr_warren.score(X_train_warren, y_train_warren)\n",
    "warren_test_acc = lr_warren.score(X_test_warren, y_test_warren)\n",
    "warren_train_f1 = f1_score(lr_warren.predict(X_test_warren), y_test_warren)\n",
    "warren_test_f1 = f1_score(lr_warren.predict(X_train_warren), y_train_warren)\n",
    "with open('evaluate/warren_evaluate.pk', 'wb') as file:\n",
    "    pickle.dump([warren_train_acc, warren_test_acc, warren_train_f1, warren_test_f1], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Train Acc\t Test Acc\t Train F1 Score\t Test F1 Score\n",
      "Warren\t 0.9828 \t 0.6833 \t 0.6772 \t 0.9829\n"
     ]
    }
   ],
   "source": [
    "print(\"\\t\",\"Train Acc\\t\", \"Test Acc\\t\", \"Train F1 Score\\t\", \"Test F1 Score\")\n",
    "print(\"Warren\\t\", '{:3.4f}'.format(warren_train_acc), \"\\t\", '{:3.4f}'.format(warren_test_acc), \"\\t\",\n",
    "      '{:3.4f}'.format(warren_train_f1), \"\\t\", '{:3.4f}'.format(warren_test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00765621,  0.46294905,  0.79182539,  1.60234741,  1.22169369,\n",
       "       -0.3738697 ,  0.13445727, -0.08218968,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_warren.coef_[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.7616293778995165, '@realDonaldTrump'),\n",
       " (1.709477065578446, 'Brett Kavanaugh'),\n",
       " (1.658493776949671, 'Coretta Scott King'),\n",
       " (1.6023474110525664, 'disgust'),\n",
       " (1.4814470638650068, 'Jeff Sessions'),\n",
       " (1.4751044536089133, 'Tonight'),\n",
       " (1.4701056189010773, 'fight'),\n",
       " (1.4381140949258584, 'Russia'),\n",
       " (1.428683080655806, 'Affordable Care Act'),\n",
       " (1.3593645411876183, 'plan')]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(zip(lr_warren.coef_[0], warren_feature_names)), key=lambda x: x[0], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9982788296041308\n"
     ]
    }
   ],
   "source": [
    "svm_warren = SVC(C=4.0, kernel='linear') # rbf -> .50, linear -> 0.652\n",
    "svm_warren.fit(X_train_warren, y_train_warren)\n",
    "svm_warren.score(X_test_warren, y_test_warren)\n",
    "f1_score(svm_warren.predict(X_test_warren), y_test_warren)\n",
    "print(f1_score(svm_warren.predict(X_train_warren), y_train_warren))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump and load to pickle file.\n",
    "with open('Predictions/warren_LR.pk', 'wb') as file:\n",
    "    pickle.dump(lr_warren, file)\n",
    "with open('Predictions/warren_SVM.pk', 'wb') as file:\n",
    "    pickle.dump(svm_warren, file)\n",
    "\n",
    "with open('Predictions/warren_LR.pk', 'rb') as file: \n",
    "    lr_warren = pickle.load(file)    \n",
    "with open('Predictions/warren_SVM.pk', 'rb') as file:    \n",
    "    svm_warren = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "biden_tweets = pickle.load(open('./tweetsent/tweets/old/biden_tweets_old.pkl', 'rb'))\n",
    "biden_senti_scores = pickle.load(open('./tweetsent/senti_scores/biden_senti_scores.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Thresholds\n",
    "num_retweets_biden = np.array([biden_tweets[i]['retweet_count']\n",
    "                               for i in range(len(biden_tweets))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding max character\n",
    "biden_max_char=0 #315\n",
    "for i in biden_tweets[0:]:\n",
    "    biden_max_char = max(biden_max_char, biden_senti_scores[i['full_text']]['usage']['text_characters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a corpus\n",
    "corpus = set()\n",
    "for tweet in biden_senti_scores:\n",
    "    corpus.update({i['text'] for i in biden_senti_scores[tweet]['keywords']})\n",
    "biden_sorted_corpus = sorted(corpus)\n",
    "\n",
    "with open('FeatureData/biden_corpus.pk', 'wb') as file:\n",
    "    pickle.dump(biden_sorted_corpus, file)\n",
    "\n",
    "with open('FeatureData/biden_corpus.pk', 'rb') as file:\n",
    "    biden_sorted_corpus = pickle.load(file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Feature Matrix\n",
    "biden_features = []\n",
    "biden_labels = []\n",
    "biden_feature_names = ['sadness', 'joy', 'fear', 'disgust', 'anger',\n",
    "                        'sentiment', 'character'] + [i for i in biden_sorted_corpus]\n",
    "\n",
    "for i in biden_tweets:\n",
    "    # Ambigious discarded Binary Labels\n",
    "    if i['retweet_count'] <= 208: #247: #302:\n",
    "        biden_labels.append(-1)\n",
    "    elif i['retweet_count'] >= 302: # 398: #784\n",
    "        biden_labels.append(1)\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    # Feature\n",
    "    tweet_feature = []\n",
    "    for j,k in biden_senti_scores[i['full_text']]['emotion']['document']['emotion'].items():\n",
    "        tweet_feature.append(k)\n",
    "    tweet_feature.append(biden_senti_scores[i['full_text']]['sentiment']['document']['score'])\n",
    "    biden_feature_names.append('sentiment')\n",
    "    \n",
    "    tweet_feature.append(biden_senti_scores[i['full_text']]['usage']['text_characters']/biden_max_char)\n",
    "    biden_feature_names.append('character')\n",
    "    \n",
    "    # One-hot Encoded Features\n",
    "    text_relevance = dict({sent['text']:sent['relevance'] for sent in biden_senti_scores[i['full_text']]['keywords']})\n",
    "    tweet_onehot=[]\n",
    "    for keys in biden_sorted_corpus:\n",
    "        \n",
    "        tweet_onehot.append(0 if keys not in text_relevance.keys() else text_relevance[keys])\n",
    "    tweet_feature.extend(tweet_onehot)\n",
    "    \n",
    "    # Add all to features matrix\n",
    "    biden_features.append(tweet_feature)\n",
    "\n",
    "with open('FeatureData/biden_features.pk', 'wb') as file:\n",
    "    pickle.dump([biden_features, biden_feature_names, biden_labels], file)\n",
    "\n",
    "with open('FeatureData/biden_features.pk', 'rb') as file:\n",
    "    biden_features, biden_feature_names, biden_labels = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-114"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(biden_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_biden, X_test_biden, y_train_biden, y_test_biden = train_test_split(biden_features, biden_labels, test_size=1/3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9408224674022067"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_biden = LogisticRegression(C=2.0)\n",
    "lr_biden.fit(X_train_biden, y_train_biden)\n",
    "lr_biden.score(X_test_biden, y_test_biden)\n",
    "f1_score(lr_biden.predict(X_test_biden), y_test_biden)\n",
    "f1_score(lr_biden.predict(X_train_biden), y_train_biden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "biden_train_acc = lr_biden.score(X_train_biden, y_train_biden)\n",
    "biden_test_acc = lr_biden.score(X_test_biden, y_test_biden)\n",
    "biden_train_f1 = f1_score(lr_biden.predict(X_test_biden), y_test_biden)\n",
    "biden_test_f1 = f1_score(lr_biden.predict(X_train_biden), y_train_biden)\n",
    "with open('evaluate/biden_evaluate.pk', 'wb') as file:\n",
    "    pickle.dump([biden_train_acc, biden_test_acc, biden_train_f1, biden_test_f1], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sadness',\n",
       " 'joy',\n",
       " 'fear',\n",
       " 'disgust',\n",
       " 'anger',\n",
       " 'sentiment',\n",
       " 'character',\n",
       " '#1010means',\n",
       " '#1is2many',\n",
       " '#AARPIowaForum']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biden_feature_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.31148925,  0.88325437,  0.81126871,  0.43724336,  0.29143923,\n",
       "       -0.32549082,  1.98244647, -0.17973047, -0.06138752,  0.        ])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_biden.coef_[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2.0865519399851644, 'Hillary'),\n",
       " (1.982446467185269, 'character'),\n",
       " (1.9547568908522612, 'tonight'),\n",
       " (1.7145053873530272, 'friend'),\n",
       " (1.5651393775243028, 'America'),\n",
       " (1.502783331318645, 'Donald Trump'),\n",
       " (1.4938376603200787, 'Jill'),\n",
       " (1.4203565942119496, 'families'),\n",
       " (1.4163040779048284, 'South Carolina'),\n",
       " (1.3767573309764107, 'President Trump')]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(zip(lr_biden.coef_[0], biden_feature_names)), key=lambda x: x[0], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9814995131450828"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_biden = SVC(C=4.0, kernel='linear') # rbf -> .86, linear -> 0.85\n",
    "svm_biden.fit(X_train_biden, y_train_biden)\n",
    "svm_biden.score(X_test_biden, y_test_biden)\n",
    "f1_score(svm_biden.predict(X_test_biden), y_test_biden)\n",
    "f1_score(svm_biden.predict(X_train_biden), y_train_biden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump and load to pickle file.\n",
    "with open('Predictions/biden_LR.pk', 'wb') as file:\n",
    "    pickle.dump(lr_biden, file)\n",
    "with open('Predictions/biden_SVM.pk', 'wb') as file:\n",
    "    pickle.dump(svm_biden, file)\n",
    "\n",
    "with open('Predictions/biden_LR.pk', 'rb') as file: \n",
    "    lr_biden = pickle.load(file)    \n",
    "with open('Predictions/biden_SVM.pk', 'rb') as file:    \n",
    "    svm_biden = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernie_tweets = pickle.load(open('./tweetsent/tweets/old/bernie_tweets_old.pkl', 'rb'))\n",
    "bernie_senti_scores = pickle.load(open('./tweetsent/senti_scores/bernie_senti_scores.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Thresholds\n",
    "num_retweets_bernie = np.array([bernie_tweets[i]['retweet_count']\n",
    "                               for i in range(len(bernie_tweets))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding max character\n",
    "bernie_max_char=0 # 304\n",
    "for i in bernie_tweets:\n",
    "    bernie_max_char = max(bernie_max_char, bernie_senti_scores[i['full_text']]['usage']['text_characters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a corpus\n",
    "corpus = set()\n",
    "for tweet in bernie_senti_scores:\n",
    "    corpus.update({i['text'] for i in bernie_senti_scores[tweet]['keywords']})\n",
    "bernie_sorted_corpus = sorted(corpus)\n",
    "\n",
    "with open('FeatureData/bernie_corpus.pk', 'wb') as file:\n",
    "    pickle.dump(bernie_sorted_corpus, file)\n",
    "    \n",
    "with open('FeatureData/bernie_corpus.pk', 'rb') as file:\n",
    "    bernie_sorted_corpus = pickle.load(file)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Feature Matrix\n",
    "bernie_features = []\n",
    "bernie_labels = []\n",
    "bernie_feature_names = ['sadness', 'joy', 'fear', 'disgust', 'anger',\n",
    "                        'sentiment', 'character'] + [i for i in bernie_sorted_corpus]\n",
    "\n",
    "for i in bernie_tweets:\n",
    "    # Ambigious discarded Binary Labels\n",
    "    if i['retweet_count'] <= 1080:\n",
    "        bernie_labels.append(-1)\n",
    "    elif i['retweet_count'] >= 1612:\n",
    "        bernie_labels.append(1)\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    # Feature\n",
    "    tweet_feature = []\n",
    "    for j,k in bernie_senti_scores[i['full_text']]['emotion']['document']['emotion'].items():\n",
    "        tweet_feature.append(k)\n",
    "    tweet_feature.append(bernie_senti_scores[i['full_text']]['sentiment']['document']['score'])\n",
    "    bernie_feature_names.append('sentiment')\n",
    "    \n",
    "    tweet_feature.append(bernie_senti_scores[i['full_text']]['usage']['text_characters']/bernie_max_char)\n",
    "    bernie_feature_names.append('character')\n",
    "    \n",
    "    # One-hot Encoded Features\n",
    "    text_relevance = dict({sent['text']:sent['relevance'] for sent in bernie_senti_scores[i['full_text']]['keywords']})\n",
    "    tweet_onehot=[]\n",
    "    for keys in bernie_sorted_corpus:\n",
    "        \n",
    "        tweet_onehot.append(0 if keys not in text_relevance.keys() else text_relevance[keys])\n",
    "    tweet_feature.extend(tweet_onehot)\n",
    "    \n",
    "    # Add all to features matrix\n",
    "    bernie_features.append(tweet_feature)\n",
    "\n",
    "with open('FeatureData/bernie_features.pk', 'wb') as file:\n",
    "    pickle.dump([bernie_features, bernie_feature_names, bernie_labels], file)\n",
    "\n",
    "with open('FeatureData/bernie_features.pk', 'rb') as file:\n",
    "    bernie_features, bernie_feature_names, bernie_labels = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-28"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(bernie_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bernie, X_test_bernie, y_train_bernie, y_test_bernie = train_test_split(bernie_features, bernie_labels, test_size=1/3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9720998531571219"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_bernie = LogisticRegression(C=2.0)\n",
    "lr_bernie.fit(X_train_bernie, y_train_bernie)\n",
    "lr_bernie.score(X_test_bernie, y_test_bernie)\n",
    "f1_score(lr_bernie.predict(X_test_bernie), y_test_bernie)\n",
    "f1_score(lr_bernie.predict(X_train_bernie), y_train_bernie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernie_train_acc = lr_bernie.score(X_train_bernie, y_train_bernie)\n",
    "bernie_test_acc = lr_bernie.score(X_test_bernie, y_test_bernie)\n",
    "bernie_train_f1 = f1_score(lr_bernie.predict(X_test_bernie), y_test_bernie)\n",
    "bernie_test_f1 = f1_score(lr_bernie.predict(X_train_bernie), y_train_bernie)\n",
    "with open('evaluate/bernie_evaluate.pk', 'wb') as file:\n",
    "    pickle.dump([bernie_train_acc, bernie_test_acc, bernie_train_f1, bernie_test_f1], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.38313301, -1.31607588,  0.06624864,  0.56532138,  0.96273504,\n",
       "       -0.0046789 , -1.3072561 , -0.14151721,  0.        ,  0.04517574])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_bernie.coef_[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.5369562219748851, 'child'),\n",
       " (1.3101264200356815, 'hour'),\n",
       " (1.2471883236022463, 'Cardi B'),\n",
       " (1.2454094871199144, 'CEO'),\n",
       " (1.2069942736005168, 'dollars'),\n",
       " (1.1846947311483609, 'Republicans'),\n",
       " (1.1550651698827914, 'today'),\n",
       " (1.144075574059014, 'racist'),\n",
       " (1.1224045548406671, '21st Century Economic Bill of Rights'),\n",
       " (1.1183580225873515, 'fact')]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(zip(lr_bernie.coef_[0], bernie_feature_names)), key=lambda x: x[0], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9970414201183432"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_bernie = SVC(C=4.0, kernel='linear') # rbf -> .86, linear -> 0.85\n",
    "svm_bernie.fit(X_train_bernie, y_train_bernie)\n",
    "svm_bernie.score(X_test_bernie, y_test_bernie)\n",
    "f1_score(svm_bernie.predict(X_test_bernie), y_test_bernie)\n",
    "f1_score(svm_bernie.predict(X_train_bernie), y_train_bernie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump and load to pickle file.\n",
    "with open('Predictions/bernie_LR.pk', 'wb') as file:\n",
    "    pickle.dump(lr_bernie, file)\n",
    "with open('Predictions/bernie_SVM.pk', 'wb') as file:\n",
    "    pickle.dump(svm_bernie, file)\n",
    "\n",
    "with open('Predictions/bernie_LR.pk', 'rb') as file: \n",
    "    lr_bernie = pickle.load(file)    \n",
    "with open('Predictions/bernie_SVM.pk', 'rb') as file:    \n",
    "    svm_bernie = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "yang_tweets = pickle.load(open('./tweetsent/tweets/old/yang_tweets_old.pkl', 'rb'))\n",
    "yang_senti_scores = pickle.load(open('./tweetsent/senti_scores/yang_senti_scores.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Thresholds\n",
    "num_retweets_yang = np.array([yang_tweets[i]['retweet_count']\n",
    "                             for i in range(len(yang_tweets))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding max character\n",
    "yang_max_char= 0 #329\n",
    "for i in yang_tweets:\n",
    "    yang_max_char = max(yang_max_char, yang_senti_scores[i['full_text']]['usage']['text_characters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a corpus\n",
    "corpus = set()\n",
    "for tweet in yang_senti_scores:\n",
    "    corpus.update({i['text'] for i in yang_senti_scores[tweet]['keywords']})\n",
    "yang_sorted_corpus = sorted(corpus)\n",
    "\n",
    "with open('FeatureData/yang_corpus.pk', 'wb') as file:\n",
    "    pickle.dump(yang_sorted_corpus, file)\n",
    "\n",
    "with open('FeatureData/yang_corpus.pk', 'rb') as file:\n",
    "    yang_sorted_corpus = pickle.load(file)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Feature Matrix\n",
    "yang_features = []\n",
    "yang_labels = []\n",
    "yang_feature_names = ['sadness', 'joy', 'fear', 'disgust', 'anger',\n",
    "                        'sentiment', 'character'] + [i for i in yang_sorted_corpus]\n",
    "\n",
    "for i in yang_tweets:\n",
    "    # Ambigious discarded Binary Labels\n",
    "    if i['retweet_count'] <= 335: #880:\n",
    "        yang_labels.append(-1)\n",
    "    elif i['retweet_count'] >= 524: #1612:\n",
    "        yang_labels.append(1)\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    # Feature\n",
    "    tweet_feature = []\n",
    "    for j,k in yang_senti_scores[i['full_text']]['emotion']['document']['emotion'].items():\n",
    "        tweet_feature.append(k)\n",
    "    tweet_feature.append(yang_senti_scores[i['full_text']]['sentiment']['document']['score'])\n",
    "    yang_feature_names.append('sentiment')\n",
    "    \n",
    "    tweet_feature.append(yang_senti_scores[i['full_text']]['usage']['text_characters']/yang_max_char)\n",
    "    yang_feature_names.append('character')\n",
    "    \n",
    "    # One-hot Encoded Features\n",
    "    text_relevance = dict({sent['text']:sent['relevance'] for sent in yang_senti_scores[i['full_text']]['keywords']})\n",
    "    tweet_onehot=[]\n",
    "    for keys in yang_sorted_corpus:\n",
    "        \n",
    "        tweet_onehot.append(0 if keys not in text_relevance.keys() else text_relevance[keys])\n",
    "    tweet_feature.extend(tweet_onehot)\n",
    "    \n",
    "    # Add all to features matrix\n",
    "    yang_features.append(tweet_feature)\n",
    "\n",
    "with open('FeatureData/yang_features.pk', 'wb') as file:\n",
    "    pickle.dump([yang_features, yang_feature_names, yang_labels], file)\n",
    "\n",
    "with open('FeatureData/yang_features.pk', 'rb') as file:\n",
    "    yang_features, yang_feature_names, yang_labels = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-16"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(yang_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_yang, X_test_yang, y_train_yang, y_test_yang = train_test_split(yang_features, yang_labels, test_size=1/3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9282442748091604"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_yang = LogisticRegression(C=2.0)\n",
    "lr_yang.fit(X_train_yang, y_train_yang)\n",
    "lr_yang.score(X_test_yang, y_test_yang)\n",
    "f1_score(lr_yang.predict(X_test_yang), y_test_yang)\n",
    "f1_score(lr_yang.predict(X_train_yang), y_train_yang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "yang_train_acc = lr_yang.score(X_train_yang, y_train_yang)\n",
    "yang_test_acc = lr_yang.score(X_test_yang, y_test_yang)\n",
    "yang_train_f1 = f1_score(lr_yang.predict(X_test_yang), y_test_yang)\n",
    "yang_test_f1 = f1_score(lr_yang.predict(X_train_yang), y_train_yang)\n",
    "with open('evaluate/yang_evaluate.pk', 'wb') as file:\n",
    "    pickle.dump([yang_train_acc, yang_test_acc, yang_train_f1, yang_test_f1], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.60769436, -0.89138319,  0.01539488,  2.22248272, -0.47985534,\n",
       "       -1.04411181,  4.4340393 ,  0.33329203,  0.22658596,  0.        ])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_yang.coef_[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4.434039303811506, 'character'),\n",
       " (2.2224827249064756, 'disgust'),\n",
       " (1.5713187667199533, 'Americans'),\n",
       " (1.3312580030438101, 'thanks'),\n",
       " (1.224293057473477, 'time'),\n",
       " (1.2216042992555196, 'People'),\n",
       " (1.2177627751945206, 'better win'),\n",
       " (1.1430301520268586, 'weed'),\n",
       " (1.130134091345457, 'Joe Biden'),\n",
       " (1.1005211217166224, 'lot')]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(zip(lr_yang.coef_[0], yang_feature_names)), key=lambda x: x[0], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9742813918305598"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_yang = SVC(C=4.0, kernel='linear') # rbf -> .86, linear -> 0.85\n",
    "svm_yang.fit(X_train_yang, y_train_yang)\n",
    "svm_yang.score(X_test_yang, y_test_yang)\n",
    "f1_score(svm_yang.predict(X_test_yang), y_test_yang)\n",
    "f1_score(svm_yang.predict(X_train_yang), y_train_yang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump and load to pickle file.\n",
    "with open('Predictions/yang_LR.pk', 'wb') as file:\n",
    "    pickle.dump(lr_yang, file)\n",
    "with open('Predictions/yang_SVM.pk', 'wb') as file:\n",
    "    pickle.dump(svm_yang, file)\n",
    "\n",
    "with open('Predictions/yang_LR.pk', 'rb') as file: \n",
    "    lr_yang = pickle.load(file)    \n",
    "with open('Predictions/yang_SVM.pk', 'rb') as file:    \n",
    "    svm_yang = pickle.load(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
